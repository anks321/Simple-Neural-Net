# Simple-Neural-Ne


This is an implementation simple neural network using pytorch on the MNIST dataset.It takes in 28*28 input size has 100 units in hidden layer and outputs into 10 unit outputlayer. ReLU non linearity function is used everywhere. Cross Entropy Loss function is used as loss function and optimizer is Adam optimizer.
#### Learning rate = 0.001
#### batch_size = 64
#### Epochs trained =1

